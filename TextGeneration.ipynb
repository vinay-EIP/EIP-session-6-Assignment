{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGeneration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNFsYgdg-PNH",
        "colab_type": "code",
        "outputId": "195952f1-cdad-4afe-d0f9-3720e6652962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!curl -o Wonderland.txt \"https://www.gutenberg.org/files/11/11-0.txt\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  169k  100  169k    0     0  84680      0  0:00:02  0:00:02 --:--:-- 84680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_tjRq2FmsO7",
        "colab_type": "text"
      },
      "source": [
        "# EIP Session 6 Assignment  (Phase-2 Session 2)\n",
        "\n",
        "\n",
        "   1. Go through this Post: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ (Links to an external site.)\n",
        "   \n",
        "   2. Add these improvements to the final code described in the post:\n",
        "    \n",
        "        a. Predict 500 characters only\n",
        "        \n",
        "        b. Remove all the punctuation from the source text\n",
        "        \n",
        "        c. Train the model on padded sequences (Links to an external site.) rather than random sequences of characters. \n",
        "        \n",
        "        d. Train the model for 100 epochs\n",
        "        \n",
        "        e. Add dropout to the input layer, remove it from the layer before dense layer. Use Dropout value of 0.1 everywhere\n",
        "        \n",
        "        f.Submit!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsKBqtKwnHjM",
        "colab_type": "text"
      },
      "source": [
        "## The data is downloaded from the official website using curl.\n",
        "\n",
        "## About dataset:\n",
        "\n",
        "Download Link : https://www.gutenberg.org/ebooks/11\n",
        "\n",
        "Alice's Adventures in Wonderland (commonly shortened to Alice in Wonderland) is an 1865 novel written by English author Charles Lutwidge Dodgson under the pseudonym Lewis Carroll. It tells of a young girl named Alice falling through a rabbit hole into a fantasy world populated by peculiar, anthropomorphic creatures.\n",
        "\n",
        "We have used the plain text UTF-8 format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKqdmZd6-oHo",
        "colab_type": "code",
        "outputId": "4f861d45-c632-45b3-e4a2-b19269d819e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  Wonderland.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK1j6kaVs_mT",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing:\n",
        "  1.  Convert entire text to lower cases (To reduce the number of unuqie characters)\n",
        "  2. Remove the unwanted new lines so that its a one single string where sentences are split by fullstops\n",
        "  3. All punctuations apart from comma and fullstops are removed\n",
        "  4. Individual characters are converted into integers based on a mapping\n",
        "  5. Prepadding is done while generating sequential information in each sentences influenced by https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\n",
        "  6. The target \"y\" is defined as the very first letter after the input \"X\" sequence in the sentence\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbLCpW3i-sMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"Wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPzcE5Dy-xi7",
        "colab_type": "code",
        "outputId": "8b8d124c-c861-4fab-c604-60e6cbb0b2ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "print(raw_text[:1000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿project gutenberg’s alice’s adventures in wonderland, by lewis carroll\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  you may copy it, give it away or\n",
            "re-use it under the terms of the project gutenberg license included\n",
            "with this ebook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "title: alice’s adventures in wonderland\n",
            "\n",
            "author: lewis carroll\n",
            "\n",
            "posting date: june 25, 2008 [ebook #11]\n",
            "release date: march, 1994\n",
            "last updated: october 6, 2016\n",
            "\n",
            "language: english\n",
            "\n",
            "character set encoding: utf-8\n",
            "\n",
            "*** start of this project gutenberg ebook alice’s adventures in wonderland ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "alice’s adventures in wonderland\n",
            "\n",
            "lewis carroll\n",
            "\n",
            "the millennium fulcrum edition 3.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "chapter i. down the rabbit-hole\n",
            "\n",
            "alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, ‘and what is the use of a book,’ tho\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQzxiy_g_a74",
        "colab_type": "code",
        "outputId": "0ebd2faf-7743-41c6-da1f-67de580892a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy\n",
        "import string\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RBnI55p_fpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modified_text = \"\"\n",
        "for i in range(0,len(raw_text)-1):\n",
        "    if raw_text[i] == '\\n':\n",
        "        if raw_text[i+1] == '\\n':\n",
        "            continue\n",
        "        elif raw_text[i-1] == '\\n':\n",
        "            modified_text += '. '\n",
        "        else:\n",
        "            modified_text += ' '\n",
        "    else:\n",
        "        modified_text += raw_text[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT3TicTSHKq1",
        "colab_type": "code",
        "outputId": "acc50597-23fd-487a-a0cd-b8c8d5c19592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(modified_text[:2000])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿project gutenberg’s alice’s adventures in wonderland, by lewis carroll. this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  you may copy it, give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org. title: alice’s adventures in wonderland. author: lewis carroll. posting date: june 25, 2008 [ebook #11] release date: march, 1994 last updated: october 6, 2016. language: english. character set encoding: utf-8. *** start of this project gutenberg ebook alice’s adventures in wonderland ***. alice’s adventures in wonderland. lewis carroll. the millennium fulcrum edition 3.0. chapter i. down the rabbit-hole. alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, ‘and what is the use of a book,’ thought alice ‘without pictures or conversations?’. so she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a white rabbit with pink eyes ran close by her.. there was nothing so very remarkable in that; nor did alice think it so very much out of the way to hear the rabbit say to itself, ‘oh dear! oh dear! i shall be late!’ (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ_AtR1mtCZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVpZL7GHQhm",
        "colab_type": "code",
        "outputId": "890a7622-862b-45e6-bdb2-8be175781594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "chars = sorted(list(set(modified_text)))\n",
        "n_chars = len(modified_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "print(\"Characters: \",chars)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  163751\n",
            "Total Vocab:  60\n",
            "Characters:  [' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3w2aEecu2Q4",
        "colab_type": "text"
      },
      "source": [
        "## Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NxDy3NWHSKV",
        "colab_type": "code",
        "outputId": "c26a6001-0bf1-4715-cfee-b978607ed9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "exclude = set(string.punctuation)\n",
        "print(\"Len: \",len(exclude))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Len:  32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjW2zZyFHU7D",
        "colab_type": "code",
        "outputId": "2ed2a9cc-830d-4e02-e0f2-c22c98230351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "exclude.remove('.')\n",
        "#exclude.remove(';')\n",
        "exclude.remove(',')\n",
        "#exclude.remove(\"'\")\n",
        "\n",
        "print(exclude)\n",
        "print(\"Len: \",len(exclude))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'{', '\\\\', '@', '[', '\"', '&', '!', '=', '~', '%', '$', '}', '_', '+', '/', ']', '*', '|', '#', '(', '-', ';', '<', '`', '^', '?', ':', ')', '>', \"'\"}\n",
            "Len:  30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzU0EzahHWrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modified_text = ''.join(ch for ch in modified_text if ch not in exclude)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-bRcYe2u59C",
        "colab_type": "text"
      },
      "source": [
        "## Character to integer mapping definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwW_CewyHaJL",
        "colab_type": "code",
        "outputId": "de6ead63-06ea-4c3e-f36b-763c2abf66b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(modified_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "n_chars = len(modified_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "print(\"Characters: \",chars)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  161621\n",
            "Total Vocab:  44\n",
            "Characters:  [' ', ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSp1tbku9zq",
        "colab_type": "text"
      },
      "source": [
        "## This is the method specified in the blog. It used randon set of continuous sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALj8TrVrINbh",
        "colab_type": "code",
        "outputId": "76cd2b2c-6d9a-4d00-c0cf-c06b1316ebee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# prepare the dataset of input to output pairs encoded as integers\\nseq_length = 100\\ndataX = []\\ndataY = []\\nfor i in range(0, n_chars - seq_length, 1):\\n    seq_in = modified_text[i:i + seq_length]\\n    seq_out = modified_text[i + seq_length]\\n    dataX.append([char_to_int[char] for char in seq_in])\\n    dataY.append(char_to_int[seq_out])\\nn_patterns = len(dataX)\\nprint(\"Total Patterns: \", n_patterns)\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CZAK0M6vF_H",
        "colab_type": "text"
      },
      "source": [
        " # Explanation of padding:\n",
        " \n",
        " Sentence: \"You are beautiful\"\n",
        " \n",
        " All patterns:\n",
        " X = \"Y\"            y = \"o\"\n",
        " \n",
        " X = \"Yo\"           y = \"u\"\n",
        " \n",
        " X = \"You\"          y = \" \"\n",
        " \n",
        " X = \"You \"         y = \"a\"\n",
        " \n",
        " and so on.... \n",
        " And each X sentence is converted into character list, mapped to integers and pre padded to 100 characters with value = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8E1c8QQNXIA",
        "colab_type": "code",
        "outputId": "3dad512a-88fe-4b06-a363-1afbc24d7a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for sentence in modified_text.split('.'):\n",
        "    for i in range(1,len(sentence)):\n",
        "        if i <= seq_length:\n",
        "            seq_in = sentence[:i]\n",
        "        else:\n",
        "            seq_in = sentence[i-seq_length:i]\n",
        "        seq_out = modified_text[i]\n",
        "        dataX.append([char_to_int[char] for char in seq_in])\n",
        "        dataY.append(char_to_int[seq_out])\n",
        "dataX = pad_sequences(dataX, maxlen=seq_length)\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  157901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwMHDIz6I4iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZgHEmGpvQK5",
        "colab_type": "text"
      },
      "source": [
        "# LSTM model definition in keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o6H0OT0wRzd",
        "colab_type": "text"
      },
      "source": [
        "## Adding drop out to the visible input layer of the first LSTM\n",
        "\n",
        "Ref: https://keras.io/layers/recurrent/\n",
        "\n",
        "The LSTM api from keras as 2 paramets namely dropout and recurrent_dropout.\n",
        "1. dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "2. recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.\n",
        "\n",
        "To achive the desired dropout at the input layer, the first parameter is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-y0Bjudu26C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_pro = Sequential()\n",
        "model_pro.add(LSTM(256,dropout=0.1, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model_pro.add(Dropout(0.1))\n",
        "model_pro.add(LSTM(256))\n",
        "#model_pro.add(Dropout(0.1))\n",
        "model_pro.add(Dense(y.shape[1], activation='softmax'))\n",
        "model_pro.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6TWa6rptjxr",
        "colab_type": "code",
        "outputId": "d9d74a3e-e482-4652-aefb-de34d7b7adbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model_pro.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 41)                10537     \n",
            "=================================================================\n",
            "Total params: 800,041\n",
            "Trainable params: 800,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRWXt9-CI-6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"TextGeneration.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNoAg3eLJIRw",
        "colab_type": "code",
        "outputId": "7376329b-0342-419a-fe9e-23ebf08e7a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_pro.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0727 05:19:54.779409 139679689156480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "157901/157901 [==============================] - 271s 2ms/step - loss: 2.9598\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.95978, saving model to TextGeneration.hdf5\n",
            "Epoch 2/100\n",
            "157901/157901 [==============================] - 266s 2ms/step - loss: 2.9691\n",
            "\n",
            "Epoch 00002: loss did not improve from 2.95978\n",
            "Epoch 3/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.9005\n",
            "\n",
            "Epoch 00003: loss improved from 2.95978 to 2.90054, saving model to TextGeneration.hdf5\n",
            "Epoch 4/100\n",
            "157901/157901 [==============================] - 263s 2ms/step - loss: 2.4742\n",
            "\n",
            "Epoch 00004: loss improved from 2.90054 to 2.47420, saving model to TextGeneration.hdf5\n",
            "Epoch 5/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 2.0570\n",
            "\n",
            "Epoch 00005: loss improved from 2.47420 to 2.05703, saving model to TextGeneration.hdf5\n",
            "Epoch 6/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.8540\n",
            "\n",
            "Epoch 00006: loss improved from 2.05703 to 1.85398, saving model to TextGeneration.hdf5\n",
            "Epoch 7/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.7289\n",
            "\n",
            "Epoch 00007: loss improved from 1.85398 to 1.72890, saving model to TextGeneration.hdf5\n",
            "Epoch 8/100\n",
            "157901/157901 [==============================] - 263s 2ms/step - loss: 1.6411\n",
            "\n",
            "Epoch 00008: loss improved from 1.72890 to 1.64110, saving model to TextGeneration.hdf5\n",
            "Epoch 9/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.5761\n",
            "\n",
            "Epoch 00009: loss improved from 1.64110 to 1.57607, saving model to TextGeneration.hdf5\n",
            "Epoch 10/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.5800\n",
            "\n",
            "Epoch 00010: loss did not improve from 1.57607\n",
            "Epoch 11/100\n",
            "157901/157901 [==============================] - 261s 2ms/step - loss: 1.5408\n",
            "\n",
            "Epoch 00011: loss improved from 1.57607 to 1.54081, saving model to TextGeneration.hdf5\n",
            "Epoch 12/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.5174\n",
            "\n",
            "Epoch 00012: loss improved from 1.54081 to 1.51742, saving model to TextGeneration.hdf5\n",
            "Epoch 13/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.5043\n",
            "\n",
            "Epoch 00013: loss improved from 1.51742 to 1.50433, saving model to TextGeneration.hdf5\n",
            "Epoch 14/100\n",
            "157901/157901 [==============================] - 261s 2ms/step - loss: 1.4785\n",
            "\n",
            "Epoch 00014: loss improved from 1.50433 to 1.47852, saving model to TextGeneration.hdf5\n",
            "Epoch 15/100\n",
            "157901/157901 [==============================] - 261s 2ms/step - loss: 1.4725\n",
            "\n",
            "Epoch 00015: loss improved from 1.47852 to 1.47254, saving model to TextGeneration.hdf5\n",
            "Epoch 16/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.4426\n",
            "\n",
            "Epoch 00016: loss improved from 1.47254 to 1.44256, saving model to TextGeneration.hdf5\n",
            "Epoch 17/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.4633\n",
            "\n",
            "Epoch 00017: loss did not improve from 1.44256\n",
            "Epoch 18/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.4509\n",
            "\n",
            "Epoch 00018: loss did not improve from 1.44256\n",
            "Epoch 19/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.4710\n",
            "\n",
            "Epoch 00019: loss did not improve from 1.44256\n",
            "Epoch 20/100\n",
            "157901/157901 [==============================] - 262s 2ms/step - loss: 1.4411\n",
            "\n",
            "Epoch 00020: loss improved from 1.44256 to 1.44111, saving model to TextGeneration.hdf5\n",
            "Epoch 21/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4315\n",
            "\n",
            "Epoch 00021: loss improved from 1.44111 to 1.43153, saving model to TextGeneration.hdf5\n",
            "Epoch 22/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.5330\n",
            "\n",
            "Epoch 00022: loss did not improve from 1.43153\n",
            "Epoch 23/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4523\n",
            "\n",
            "Epoch 00023: loss did not improve from 1.43153\n",
            "Epoch 24/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4105\n",
            "\n",
            "Epoch 00024: loss improved from 1.43153 to 1.41054, saving model to TextGeneration.hdf5\n",
            "Epoch 25/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4693\n",
            "\n",
            "Epoch 00025: loss did not improve from 1.41054\n",
            "Epoch 26/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4024\n",
            "\n",
            "Epoch 00026: loss improved from 1.41054 to 1.40235, saving model to TextGeneration.hdf5\n",
            "Epoch 27/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3937\n",
            "\n",
            "Epoch 00027: loss improved from 1.40235 to 1.39369, saving model to TextGeneration.hdf5\n",
            "Epoch 28/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3828\n",
            "\n",
            "Epoch 00028: loss improved from 1.39369 to 1.38284, saving model to TextGeneration.hdf5\n",
            "Epoch 29/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4455\n",
            "\n",
            "Epoch 00029: loss did not improve from 1.38284\n",
            "Epoch 30/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3940\n",
            "\n",
            "Epoch 00030: loss did not improve from 1.38284\n",
            "Epoch 31/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3778\n",
            "\n",
            "Epoch 00031: loss improved from 1.38284 to 1.37781, saving model to TextGeneration.hdf5\n",
            "Epoch 32/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3638\n",
            "\n",
            "Epoch 00032: loss improved from 1.37781 to 1.36378, saving model to TextGeneration.hdf5\n",
            "Epoch 33/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3563\n",
            "\n",
            "Epoch 00033: loss improved from 1.36378 to 1.35633, saving model to TextGeneration.hdf5\n",
            "Epoch 34/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3606\n",
            "\n",
            "Epoch 00034: loss did not improve from 1.35633\n",
            "Epoch 35/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3548\n",
            "\n",
            "Epoch 00035: loss improved from 1.35633 to 1.35479, saving model to TextGeneration.hdf5\n",
            "Epoch 36/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3632\n",
            "\n",
            "Epoch 00036: loss did not improve from 1.35479\n",
            "Epoch 37/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3579\n",
            "\n",
            "Epoch 00037: loss did not improve from 1.35479\n",
            "Epoch 38/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3397\n",
            "\n",
            "Epoch 00038: loss improved from 1.35479 to 1.33967, saving model to TextGeneration.hdf5\n",
            "Epoch 39/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3707\n",
            "\n",
            "Epoch 00039: loss did not improve from 1.33967\n",
            "Epoch 40/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3393\n",
            "\n",
            "Epoch 00040: loss improved from 1.33967 to 1.33927, saving model to TextGeneration.hdf5\n",
            "Epoch 41/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3069\n",
            "\n",
            "Epoch 00041: loss improved from 1.33927 to 1.30693, saving model to TextGeneration.hdf5\n",
            "Epoch 42/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3024\n",
            "\n",
            "Epoch 00042: loss improved from 1.30693 to 1.30236, saving model to TextGeneration.hdf5\n",
            "Epoch 43/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2863\n",
            "\n",
            "Epoch 00043: loss improved from 1.30236 to 1.28633, saving model to TextGeneration.hdf5\n",
            "Epoch 44/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2851\n",
            "\n",
            "Epoch 00044: loss improved from 1.28633 to 1.28511, saving model to TextGeneration.hdf5\n",
            "Epoch 45/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.3527\n",
            "\n",
            "Epoch 00045: loss did not improve from 1.28511\n",
            "Epoch 46/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.4284\n",
            "\n",
            "Epoch 00046: loss did not improve from 1.28511\n",
            "Epoch 47/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3140\n",
            "\n",
            "Epoch 00047: loss did not improve from 1.28511\n",
            "Epoch 48/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2984\n",
            "\n",
            "Epoch 00048: loss did not improve from 1.28511\n",
            "Epoch 49/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2713\n",
            "\n",
            "Epoch 00049: loss improved from 1.28511 to 1.27126, saving model to TextGeneration.hdf5\n",
            "Epoch 50/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2628\n",
            "\n",
            "Epoch 00050: loss improved from 1.27126 to 1.26277, saving model to TextGeneration.hdf5\n",
            "Epoch 51/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2441\n",
            "\n",
            "Epoch 00051: loss improved from 1.26277 to 1.24413, saving model to TextGeneration.hdf5\n",
            "Epoch 52/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2422\n",
            "\n",
            "Epoch 00052: loss improved from 1.24413 to 1.24224, saving model to TextGeneration.hdf5\n",
            "Epoch 53/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2311\n",
            "\n",
            "Epoch 00053: loss improved from 1.24224 to 1.23115, saving model to TextGeneration.hdf5\n",
            "Epoch 54/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2283\n",
            "\n",
            "Epoch 00054: loss improved from 1.23115 to 1.22830, saving model to TextGeneration.hdf5\n",
            "Epoch 55/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2286\n",
            "\n",
            "Epoch 00055: loss did not improve from 1.22830\n",
            "Epoch 56/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2175\n",
            "\n",
            "Epoch 00056: loss improved from 1.22830 to 1.21746, saving model to TextGeneration.hdf5\n",
            "Epoch 57/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2160\n",
            "\n",
            "Epoch 00057: loss improved from 1.21746 to 1.21600, saving model to TextGeneration.hdf5\n",
            "Epoch 58/100\n",
            "157901/157901 [==============================] - 270s 2ms/step - loss: 1.2183\n",
            "\n",
            "Epoch 00058: loss did not improve from 1.21600\n",
            "Epoch 59/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.7417\n",
            "\n",
            "Epoch 00059: loss did not improve from 1.21600\n",
            "Epoch 60/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.7090\n",
            "\n",
            "Epoch 00060: loss did not improve from 1.21600\n",
            "Epoch 61/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.6549\n",
            "\n",
            "Epoch 00061: loss did not improve from 1.21600\n",
            "Epoch 62/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.7431\n",
            "\n",
            "Epoch 00062: loss did not improve from 1.21600\n",
            "Epoch 63/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.4709\n",
            "\n",
            "Epoch 00063: loss did not improve from 1.21600\n",
            "Epoch 64/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.1317\n",
            "\n",
            "Epoch 00064: loss did not improve from 1.21600\n",
            "Epoch 65/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.8043\n",
            "\n",
            "Epoch 00065: loss did not improve from 1.21600\n",
            "Epoch 66/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.5295\n",
            "\n",
            "Epoch 00066: loss did not improve from 1.21600\n",
            "Epoch 67/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.9894\n",
            "\n",
            "Epoch 00067: loss did not improve from 1.21600\n",
            "Epoch 68/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.7772\n",
            "\n",
            "Epoch 00068: loss did not improve from 1.21600\n",
            "Epoch 69/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.6639\n",
            "\n",
            "Epoch 00069: loss did not improve from 1.21600\n",
            "Epoch 70/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.5970\n",
            "\n",
            "Epoch 00070: loss did not improve from 1.21600\n",
            "Epoch 71/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 2.5508\n",
            "\n",
            "Epoch 00071: loss did not improve from 1.21600\n",
            "Epoch 72/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.8107\n",
            "\n",
            "Epoch 00072: loss did not improve from 1.21600\n",
            "Epoch 73/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.6492\n",
            "\n",
            "Epoch 00073: loss did not improve from 1.21600\n",
            "Epoch 74/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.7499\n",
            "\n",
            "Epoch 00074: loss did not improve from 1.21600\n",
            "Epoch 75/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.4884\n",
            "\n",
            "Epoch 00075: loss did not improve from 1.21600\n",
            "Epoch 76/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.4653\n",
            "\n",
            "Epoch 00076: loss did not improve from 1.21600\n",
            "Epoch 77/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.4402\n",
            "\n",
            "Epoch 00077: loss did not improve from 1.21600\n",
            "Epoch 78/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.4089\n",
            "\n",
            "Epoch 00078: loss did not improve from 1.21600\n",
            "Epoch 79/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.9514\n",
            "\n",
            "Epoch 00079: loss did not improve from 1.21600\n",
            "Epoch 80/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.4622\n",
            "\n",
            "Epoch 00080: loss did not improve from 1.21600\n",
            "Epoch 81/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3866\n",
            "\n",
            "Epoch 00081: loss did not improve from 1.21600\n",
            "Epoch 82/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3680\n",
            "\n",
            "Epoch 00082: loss did not improve from 1.21600\n",
            "Epoch 83/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3509\n",
            "\n",
            "Epoch 00083: loss did not improve from 1.21600\n",
            "Epoch 84/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3337\n",
            "\n",
            "Epoch 00084: loss did not improve from 1.21600\n",
            "Epoch 85/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3216\n",
            "\n",
            "Epoch 00085: loss did not improve from 1.21600\n",
            "Epoch 86/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3075\n",
            "\n",
            "Epoch 00086: loss did not improve from 1.21600\n",
            "Epoch 87/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2921\n",
            "\n",
            "Epoch 00087: loss did not improve from 1.21600\n",
            "Epoch 88/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.2757\n",
            "\n",
            "Epoch 00088: loss did not improve from 1.21600\n",
            "Epoch 89/100\n",
            "157901/157901 [==============================] - 264s 2ms/step - loss: 1.5899\n",
            "\n",
            "Epoch 00089: loss did not improve from 1.21600\n",
            "Epoch 90/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3634\n",
            "\n",
            "Epoch 00090: loss did not improve from 1.21600\n",
            "Epoch 91/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2979\n",
            "\n",
            "Epoch 00091: loss did not improve from 1.21600\n",
            "Epoch 92/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2754\n",
            "\n",
            "Epoch 00092: loss did not improve from 1.21600\n",
            "Epoch 93/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.5127\n",
            "\n",
            "Epoch 00093: loss did not improve from 1.21600\n",
            "Epoch 94/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2748\n",
            "\n",
            "Epoch 00094: loss did not improve from 1.21600\n",
            "Epoch 95/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2469\n",
            "\n",
            "Epoch 00095: loss did not improve from 1.21600\n",
            "Epoch 96/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.3685\n",
            "\n",
            "Epoch 00096: loss did not improve from 1.21600\n",
            "Epoch 97/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2384\n",
            "\n",
            "Epoch 00097: loss did not improve from 1.21600\n",
            "Epoch 98/100\n",
            "157901/157901 [==============================] - 265s 2ms/step - loss: 1.2321\n",
            "\n",
            "Epoch 00098: loss did not improve from 1.21600\n",
            "Epoch 99/100\n",
            "157901/157901 [==============================] - 266s 2ms/step - loss: 1.2348\n",
            "\n",
            "Epoch 00099: loss did not improve from 1.21600\n",
            "Epoch 100/100\n",
            "  8192/157901 [>.............................] - ETA: 4:12 - loss: 1.2434Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_I0XVDlvVOG",
        "colab_type": "text"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuclqsg1mGgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f5b68b48-afa6-49dc-9b37-c70b0522dabb"
      },
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model_pro.save_weights(\"TextGeneraion_Weights_Best.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "# Save the trained model in to .h5 format\n",
        "model_pro.save(\"TextGeneration_Model_Best.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCsYFvmTvYk0",
        "colab_type": "text"
      },
      "source": [
        "# Testing the model for 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIcjsxVEMiXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "bc2e65d4-0798-4463-c0b5-2e1746890d3a"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = list(dataX[start])\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"\\n\")\n",
        "end_res = ''\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model_pro.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    end_res += result\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    #print(result, end = \"\")\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(end_res)\n",
        "print(\"\\n\")\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\"  this time she found a little bottle on it, ‘which certainly was not here before,’ said alice, and r \"\n",
            "\n",
            "\n",
            "                                                                                                    project gutebberg’s alice’s adventures in wonderland, by lewis carroll. this bbook is for the see o                                          pr                                                        dddddddddllld       arreessssea e     d o   ellnd,, y  lewis carlll   t soo    ii  oo  th                                                                                            o   o   gutenberg’s \n",
            "\n",
            "\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0X_IJbWisuo",
        "colab_type": "text"
      },
      "source": [
        "# Using lower sequence length and verifing the effect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpJA1Uo-iqzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58fef615-5cfe-4ede-da5a-9a772c410019"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 20\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = modified_text[i:i + seq_length]\n",
        "    seq_out = modified_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  161601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICUqpf3uirhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlmFrloEirbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_pro = Sequential()\n",
        "model_pro.add(LSTM(256,dropout=0.1, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model_pro.add(Dropout(0.1))\n",
        "model_pro.add(LSTM(256))\n",
        "#model_pro.add(Dropout(0.1))\n",
        "model_pro.add(Dense(y.shape[1], activation='softmax'))\n",
        "model_pro.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# define the checkpoint\n",
        "filepath=\"TextGeneration_1.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf6CH91tirWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfdfedcf-e133-4212-f0a4-18ac3602d8ee"
      },
      "source": [
        "model_pro.fit(X, y, epochs=100, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "161601/161601 [==============================] - 67s 414us/step - loss: 2.8996\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.89965, saving model to TextGeneration_1.hdf5\n",
            "Epoch 2/100\n",
            "161601/161601 [==============================] - 65s 400us/step - loss: 2.6599\n",
            "\n",
            "Epoch 00002: loss improved from 2.89965 to 2.65987, saving model to TextGeneration_1.hdf5\n",
            "Epoch 3/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 2.4879\n",
            "\n",
            "Epoch 00003: loss improved from 2.65987 to 2.48789, saving model to TextGeneration_1.hdf5\n",
            "Epoch 4/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 2.3231\n",
            "\n",
            "Epoch 00004: loss improved from 2.48789 to 2.32307, saving model to TextGeneration_1.hdf5\n",
            "Epoch 5/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 2.1842\n",
            "\n",
            "Epoch 00005: loss improved from 2.32307 to 2.18418, saving model to TextGeneration_1.hdf5\n",
            "Epoch 6/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 2.0810\n",
            "\n",
            "Epoch 00006: loss improved from 2.18418 to 2.08098, saving model to TextGeneration_1.hdf5\n",
            "Epoch 7/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 2.0507\n",
            "\n",
            "Epoch 00007: loss improved from 2.08098 to 2.05066, saving model to TextGeneration_1.hdf5\n",
            "Epoch 8/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 2.0102\n",
            "\n",
            "Epoch 00008: loss improved from 2.05066 to 2.01021, saving model to TextGeneration_1.hdf5\n",
            "Epoch 9/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.9550\n",
            "\n",
            "Epoch 00009: loss improved from 2.01021 to 1.95504, saving model to TextGeneration_1.hdf5\n",
            "Epoch 10/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.9041\n",
            "\n",
            "Epoch 00010: loss improved from 1.95504 to 1.90410, saving model to TextGeneration_1.hdf5\n",
            "Epoch 11/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.8591\n",
            "\n",
            "Epoch 00011: loss improved from 1.90410 to 1.85914, saving model to TextGeneration_1.hdf5\n",
            "Epoch 12/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.8164\n",
            "\n",
            "Epoch 00012: loss improved from 1.85914 to 1.81638, saving model to TextGeneration_1.hdf5\n",
            "Epoch 13/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 1.7787\n",
            "\n",
            "Epoch 00013: loss improved from 1.81638 to 1.77869, saving model to TextGeneration_1.hdf5\n",
            "Epoch 14/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.7430\n",
            "\n",
            "Epoch 00014: loss improved from 1.77869 to 1.74305, saving model to TextGeneration_1.hdf5\n",
            "Epoch 15/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.7094\n",
            "\n",
            "Epoch 00015: loss improved from 1.74305 to 1.70936, saving model to TextGeneration_1.hdf5\n",
            "Epoch 16/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.6789\n",
            "\n",
            "Epoch 00016: loss improved from 1.70936 to 1.67893, saving model to TextGeneration_1.hdf5\n",
            "Epoch 17/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.6483\n",
            "\n",
            "Epoch 00017: loss improved from 1.67893 to 1.64827, saving model to TextGeneration_1.hdf5\n",
            "Epoch 18/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.6180\n",
            "\n",
            "Epoch 00018: loss improved from 1.64827 to 1.61796, saving model to TextGeneration_1.hdf5\n",
            "Epoch 19/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.5892\n",
            "\n",
            "Epoch 00019: loss improved from 1.61796 to 1.58920, saving model to TextGeneration_1.hdf5\n",
            "Epoch 20/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.5595\n",
            "\n",
            "Epoch 00020: loss improved from 1.58920 to 1.55954, saving model to TextGeneration_1.hdf5\n",
            "Epoch 21/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.5347\n",
            "\n",
            "Epoch 00021: loss improved from 1.55954 to 1.53467, saving model to TextGeneration_1.hdf5\n",
            "Epoch 22/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.5085\n",
            "\n",
            "Epoch 00022: loss improved from 1.53467 to 1.50854, saving model to TextGeneration_1.hdf5\n",
            "Epoch 23/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.4842\n",
            "\n",
            "Epoch 00023: loss improved from 1.50854 to 1.48417, saving model to TextGeneration_1.hdf5\n",
            "Epoch 24/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.4561\n",
            "\n",
            "Epoch 00024: loss improved from 1.48417 to 1.45608, saving model to TextGeneration_1.hdf5\n",
            "Epoch 25/100\n",
            "161601/161601 [==============================] - 66s 405us/step - loss: 1.4377\n",
            "\n",
            "Epoch 00025: loss improved from 1.45608 to 1.43771, saving model to TextGeneration_1.hdf5\n",
            "Epoch 26/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.4098\n",
            "\n",
            "Epoch 00026: loss improved from 1.43771 to 1.40979, saving model to TextGeneration_1.hdf5\n",
            "Epoch 27/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.3829\n",
            "\n",
            "Epoch 00027: loss improved from 1.40979 to 1.38292, saving model to TextGeneration_1.hdf5\n",
            "Epoch 28/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.3586\n",
            "\n",
            "Epoch 00028: loss improved from 1.38292 to 1.35862, saving model to TextGeneration_1.hdf5\n",
            "Epoch 29/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.3405\n",
            "\n",
            "Epoch 00029: loss improved from 1.35862 to 1.34045, saving model to TextGeneration_1.hdf5\n",
            "Epoch 30/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.3171\n",
            "\n",
            "Epoch 00030: loss improved from 1.34045 to 1.31707, saving model to TextGeneration_1.hdf5\n",
            "Epoch 31/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.2959\n",
            "\n",
            "Epoch 00031: loss improved from 1.31707 to 1.29586, saving model to TextGeneration_1.hdf5\n",
            "Epoch 32/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.2702\n",
            "\n",
            "Epoch 00032: loss improved from 1.29586 to 1.27018, saving model to TextGeneration_1.hdf5\n",
            "Epoch 33/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.2530\n",
            "\n",
            "Epoch 00033: loss improved from 1.27018 to 1.25298, saving model to TextGeneration_1.hdf5\n",
            "Epoch 34/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.2291\n",
            "\n",
            "Epoch 00034: loss improved from 1.25298 to 1.22913, saving model to TextGeneration_1.hdf5\n",
            "Epoch 35/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 1.2128\n",
            "\n",
            "Epoch 00035: loss improved from 1.22913 to 1.21279, saving model to TextGeneration_1.hdf5\n",
            "Epoch 36/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.1913\n",
            "\n",
            "Epoch 00036: loss improved from 1.21279 to 1.19133, saving model to TextGeneration_1.hdf5\n",
            "Epoch 37/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.1721\n",
            "\n",
            "Epoch 00037: loss improved from 1.19133 to 1.17207, saving model to TextGeneration_1.hdf5\n",
            "Epoch 38/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.1588\n",
            "\n",
            "Epoch 00038: loss improved from 1.17207 to 1.15885, saving model to TextGeneration_1.hdf5\n",
            "Epoch 39/100\n",
            "161601/161601 [==============================] - 66s 405us/step - loss: 1.1372\n",
            "\n",
            "Epoch 00039: loss improved from 1.15885 to 1.13719, saving model to TextGeneration_1.hdf5\n",
            "Epoch 40/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.1204\n",
            "\n",
            "Epoch 00040: loss improved from 1.13719 to 1.12045, saving model to TextGeneration_1.hdf5\n",
            "Epoch 41/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.0999\n",
            "\n",
            "Epoch 00041: loss improved from 1.12045 to 1.09992, saving model to TextGeneration_1.hdf5\n",
            "Epoch 42/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 1.0844\n",
            "\n",
            "Epoch 00042: loss improved from 1.09992 to 1.08441, saving model to TextGeneration_1.hdf5\n",
            "Epoch 43/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 1.0679\n",
            "\n",
            "Epoch 00043: loss improved from 1.08441 to 1.06788, saving model to TextGeneration_1.hdf5\n",
            "Epoch 44/100\n",
            "161601/161601 [==============================] - 66s 408us/step - loss: 1.0532\n",
            "\n",
            "Epoch 00044: loss improved from 1.06788 to 1.05319, saving model to TextGeneration_1.hdf5\n",
            "Epoch 45/100\n",
            "161601/161601 [==============================] - 66s 405us/step - loss: 1.0357\n",
            "\n",
            "Epoch 00045: loss improved from 1.05319 to 1.03572, saving model to TextGeneration_1.hdf5\n",
            "Epoch 46/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.0230\n",
            "\n",
            "Epoch 00046: loss improved from 1.03572 to 1.02303, saving model to TextGeneration_1.hdf5\n",
            "Epoch 47/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 1.0071\n",
            "\n",
            "Epoch 00047: loss improved from 1.02303 to 1.00710, saving model to TextGeneration_1.hdf5\n",
            "Epoch 48/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.9967\n",
            "\n",
            "Epoch 00048: loss improved from 1.00710 to 0.99669, saving model to TextGeneration_1.hdf5\n",
            "Epoch 49/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.9783\n",
            "\n",
            "Epoch 00049: loss improved from 0.99669 to 0.97832, saving model to TextGeneration_1.hdf5\n",
            "Epoch 50/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 0.9659\n",
            "\n",
            "Epoch 00050: loss improved from 0.97832 to 0.96591, saving model to TextGeneration_1.hdf5\n",
            "Epoch 51/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.9514\n",
            "\n",
            "Epoch 00051: loss improved from 0.96591 to 0.95141, saving model to TextGeneration_1.hdf5\n",
            "Epoch 52/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.9402\n",
            "\n",
            "Epoch 00052: loss improved from 0.95141 to 0.94019, saving model to TextGeneration_1.hdf5\n",
            "Epoch 53/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.9286\n",
            "\n",
            "Epoch 00053: loss improved from 0.94019 to 0.92857, saving model to TextGeneration_1.hdf5\n",
            "Epoch 54/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.9175\n",
            "\n",
            "Epoch 00054: loss improved from 0.92857 to 0.91753, saving model to TextGeneration_1.hdf5\n",
            "Epoch 55/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.9054\n",
            "\n",
            "Epoch 00055: loss improved from 0.91753 to 0.90543, saving model to TextGeneration_1.hdf5\n",
            "Epoch 56/100\n",
            "161601/161601 [==============================] - 66s 405us/step - loss: 0.8944\n",
            "\n",
            "Epoch 00056: loss improved from 0.90543 to 0.89435, saving model to TextGeneration_1.hdf5\n",
            "Epoch 57/100\n",
            "161601/161601 [==============================] - 65s 400us/step - loss: 0.8916\n",
            "\n",
            "Epoch 00057: loss improved from 0.89435 to 0.89158, saving model to TextGeneration_1.hdf5\n",
            "Epoch 58/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.8730\n",
            "\n",
            "Epoch 00058: loss improved from 0.89158 to 0.87304, saving model to TextGeneration_1.hdf5\n",
            "Epoch 59/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.8642\n",
            "\n",
            "Epoch 00059: loss improved from 0.87304 to 0.86422, saving model to TextGeneration_1.hdf5\n",
            "Epoch 60/100\n",
            "161601/161601 [==============================] - 66s 407us/step - loss: 0.8556\n",
            "\n",
            "Epoch 00060: loss improved from 0.86422 to 0.85562, saving model to TextGeneration_1.hdf5\n",
            "Epoch 61/100\n",
            "161601/161601 [==============================] - 66s 406us/step - loss: 0.8402\n",
            "\n",
            "Epoch 00061: loss improved from 0.85562 to 0.84022, saving model to TextGeneration_1.hdf5\n",
            "Epoch 62/100\n",
            "161601/161601 [==============================] - 66s 407us/step - loss: 0.8308\n",
            "\n",
            "Epoch 00062: loss improved from 0.84022 to 0.83083, saving model to TextGeneration_1.hdf5\n",
            "Epoch 63/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.8247\n",
            "\n",
            "Epoch 00063: loss improved from 0.83083 to 0.82468, saving model to TextGeneration_1.hdf5\n",
            "Epoch 64/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.8151\n",
            "\n",
            "Epoch 00064: loss improved from 0.82468 to 0.81513, saving model to TextGeneration_1.hdf5\n",
            "Epoch 65/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.8057\n",
            "\n",
            "Epoch 00065: loss improved from 0.81513 to 0.80568, saving model to TextGeneration_1.hdf5\n",
            "Epoch 66/100\n",
            "161601/161601 [==============================] - 64s 399us/step - loss: 0.8005\n",
            "\n",
            "Epoch 00066: loss improved from 0.80568 to 0.80054, saving model to TextGeneration_1.hdf5\n",
            "Epoch 67/100\n",
            "161601/161601 [==============================] - 64s 396us/step - loss: 0.7898\n",
            "\n",
            "Epoch 00067: loss improved from 0.80054 to 0.78978, saving model to TextGeneration_1.hdf5\n",
            "Epoch 68/100\n",
            "161601/161601 [==============================] - 64s 395us/step - loss: 0.7784\n",
            "\n",
            "Epoch 00068: loss improved from 0.78978 to 0.77838, saving model to TextGeneration_1.hdf5\n",
            "Epoch 69/100\n",
            "161601/161601 [==============================] - 64s 399us/step - loss: 0.7725\n",
            "\n",
            "Epoch 00069: loss improved from 0.77838 to 0.77253, saving model to TextGeneration_1.hdf5\n",
            "Epoch 70/100\n",
            "161601/161601 [==============================] - 64s 399us/step - loss: 0.7682\n",
            "\n",
            "Epoch 00070: loss improved from 0.77253 to 0.76815, saving model to TextGeneration_1.hdf5\n",
            "Epoch 71/100\n",
            "161601/161601 [==============================] - 65s 399us/step - loss: 0.7615\n",
            "\n",
            "Epoch 00071: loss improved from 0.76815 to 0.76146, saving model to TextGeneration_1.hdf5\n",
            "Epoch 72/100\n",
            "161601/161601 [==============================] - 65s 399us/step - loss: 0.7545\n",
            "\n",
            "Epoch 00072: loss improved from 0.76146 to 0.75446, saving model to TextGeneration_1.hdf5\n",
            "Epoch 73/100\n",
            "161601/161601 [==============================] - 65s 400us/step - loss: 0.7430\n",
            "\n",
            "Epoch 00073: loss improved from 0.75446 to 0.74299, saving model to TextGeneration_1.hdf5\n",
            "Epoch 74/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.7354\n",
            "\n",
            "Epoch 00074: loss improved from 0.74299 to 0.73541, saving model to TextGeneration_1.hdf5\n",
            "Epoch 75/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.7317\n",
            "\n",
            "Epoch 00075: loss improved from 0.73541 to 0.73167, saving model to TextGeneration_1.hdf5\n",
            "Epoch 76/100\n",
            "161601/161601 [==============================] - 65s 401us/step - loss: 0.7265\n",
            "\n",
            "Epoch 00076: loss improved from 0.73167 to 0.72654, saving model to TextGeneration_1.hdf5\n",
            "Epoch 77/100\n",
            "161601/161601 [==============================] - 65s 400us/step - loss: 0.7183\n",
            "\n",
            "Epoch 00077: loss improved from 0.72654 to 0.71826, saving model to TextGeneration_1.hdf5\n",
            "Epoch 78/100\n",
            "161601/161601 [==============================] - 65s 401us/step - loss: 0.7176\n",
            "\n",
            "Epoch 00078: loss improved from 0.71826 to 0.71763, saving model to TextGeneration_1.hdf5\n",
            "Epoch 79/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.7081\n",
            "\n",
            "Epoch 00079: loss improved from 0.71763 to 0.70811, saving model to TextGeneration_1.hdf5\n",
            "Epoch 80/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.7021\n",
            "\n",
            "Epoch 00080: loss improved from 0.70811 to 0.70208, saving model to TextGeneration_1.hdf5\n",
            "Epoch 81/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.7040\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.70208\n",
            "Epoch 82/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.6916\n",
            "\n",
            "Epoch 00082: loss improved from 0.70208 to 0.69161, saving model to TextGeneration_1.hdf5\n",
            "Epoch 83/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.6903\n",
            "\n",
            "Epoch 00083: loss improved from 0.69161 to 0.69034, saving model to TextGeneration_1.hdf5\n",
            "Epoch 84/100\n",
            "161601/161601 [==============================] - 65s 405us/step - loss: 0.6784\n",
            "\n",
            "Epoch 00084: loss improved from 0.69034 to 0.67838, saving model to TextGeneration_1.hdf5\n",
            "Epoch 85/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.6766\n",
            "\n",
            "Epoch 00085: loss improved from 0.67838 to 0.67665, saving model to TextGeneration_1.hdf5\n",
            "Epoch 86/100\n",
            "161601/161601 [==============================] - 65s 401us/step - loss: 0.6767\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.67665\n",
            "Epoch 87/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.6646\n",
            "\n",
            "Epoch 00087: loss improved from 0.67665 to 0.66458, saving model to TextGeneration_1.hdf5\n",
            "Epoch 88/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.6682\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.66458\n",
            "Epoch 89/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.6509\n",
            "\n",
            "Epoch 00089: loss improved from 0.66458 to 0.65091, saving model to TextGeneration_1.hdf5\n",
            "Epoch 90/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.6497\n",
            "\n",
            "Epoch 00090: loss improved from 0.65091 to 0.64971, saving model to TextGeneration_1.hdf5\n",
            "Epoch 91/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.6545\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.64971\n",
            "Epoch 92/100\n",
            "161601/161601 [==============================] - 65s 401us/step - loss: 0.6425\n",
            "\n",
            "Epoch 00092: loss improved from 0.64971 to 0.64252, saving model to TextGeneration_1.hdf5\n",
            "Epoch 93/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.6430\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.64252\n",
            "Epoch 94/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.6404\n",
            "\n",
            "Epoch 00094: loss improved from 0.64252 to 0.64043, saving model to TextGeneration_1.hdf5\n",
            "Epoch 95/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.6337\n",
            "\n",
            "Epoch 00095: loss improved from 0.64043 to 0.63366, saving model to TextGeneration_1.hdf5\n",
            "Epoch 96/100\n",
            "161601/161601 [==============================] - 65s 403us/step - loss: 0.6283\n",
            "\n",
            "Epoch 00096: loss improved from 0.63366 to 0.62834, saving model to TextGeneration_1.hdf5\n",
            "Epoch 97/100\n",
            "161601/161601 [==============================] - 65s 404us/step - loss: 0.6217\n",
            "\n",
            "Epoch 00097: loss improved from 0.62834 to 0.62173, saving model to TextGeneration_1.hdf5\n",
            "Epoch 98/100\n",
            "161601/161601 [==============================] - 65s 401us/step - loss: 0.6203\n",
            "\n",
            "Epoch 00098: loss improved from 0.62173 to 0.62026, saving model to TextGeneration_1.hdf5\n",
            "Epoch 99/100\n",
            "161601/161601 [==============================] - 64s 399us/step - loss: 0.6156\n",
            "\n",
            "Epoch 00099: loss improved from 0.62026 to 0.61555, saving model to TextGeneration_1.hdf5\n",
            "Epoch 100/100\n",
            "161601/161601 [==============================] - 65s 402us/step - loss: 0.6095\n",
            "\n",
            "Epoch 00100: loss improved from 0.61555 to 0.60947, saving model to TextGeneration_1.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f096041e358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1bWt3C0irQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "85a42df7-8d7c-4d12-bc65-9020e03d5fdd"
      },
      "source": [
        "# Save the trained weights in to .h5 format\n",
        "model_pro.save_weights(\"TextGeneraion_1_Weights_Best.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "\n",
        "# Save the trained model in to .h5 format\n",
        "model_pro.save(\"TextGeneration_1_Model_Best.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcRvWA4Aiqt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "21126d72-7765-4f5c-8461-b020b0211f58"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = list(dataX[start])\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"\\n\")\n",
        "end_res = ''\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model_pro.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    end_res += result\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    #print(result, end = \"\")\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(end_res)\n",
        "print(\"\\n\")\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" phasis, looking hard \"\n",
            "\n",
            "\n",
            " in c hare wroembi, rnthiz yhem ier hergr  andrhe pa kovt it  anoca dow mnterflat.’ert onte alice, andrie tetlap. su sle wetts ofcrt thap.’or’t  itencutallne       i’ tme,eisce ma’s tr onte’id cnxergsmed tuadtisgipe foassurinati. hirgat inpirakd.’hy, tle felling outenf, yhir tuove whemswle tes,tn the had a vhg ootk abok sp the toat chgs in the’nnnt  and uatie  and soly toontiyed tudetingsta cre fonleted tuclte su oes.oo tif eeien allce ln a hueate inplan inmoth, andr rf tuuject gute bnice’s ard \n",
            "\n",
            "\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLnfl3mVvcc4",
        "colab_type": "text"
      },
      "source": [
        "# Conclutions:\n",
        "\n",
        "## 1. When the punctuation like \" ' \" and \" \" \" and still retained in the data, the model was able to predict direct speech sentences as well. SO work can be done on this as well\n",
        "\n",
        "## 2. Character level text generation will require larger network and more epochs to learn well. So the idea of iteration through the sentence was helpful and verified\n",
        "\n",
        "## 3. Pre padding sequences actually didn't help as it gives too many unwanted white spaces in its outout\n",
        "\n",
        "## 4. When using smaller sequences, say 20 instead of 100 (as done in the second case), the output text predected in that proper both semantically and grammatically\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9wd8tVThW07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}